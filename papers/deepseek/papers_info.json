{
  "2502.07846v1": {
    "title": "Memory Analysis on the Training Course of DeepSeek Models",
    "authors": [
      "Ping Zhang",
      "Lei Su"
    ],
    "summary": "We present a theoretical analysis of GPU memory consumption during the\ntraining of DeepSeek models such as DeepSeek-v2 and DeepSeek-v3. Our primary\nobjective is to clarify the device-level memory requirements associated with\nvarious distributed training configurations. Specifically, we examine critical\nfactors influencing memory usage, including micro-batch size, activation\nrecomputation policies, 3D parallelism, and ZeRO optimizations. It is important\nto emphasize that the training policies discussed in this report are not\nrepresentative of DeepSeek's official configurations. Instead, they are\nexplored to provide a deeper understanding of memory dynamics in training of\nlarge-scale mixture-of-experts model.",
    "pdf_url": "http://arxiv.org/pdf/2502.07846v1",
    "published": "2025-02-11"
  },
  "2503.00624v1": {
    "title": "An evaluation of DeepSeek Models in Biomedical Natural Language Processing",
    "authors": [
      "Zaifu Zhan",
      "Shuang Zhou",
      "Huixue Zhou",
      "Jiawen Deng",
      "Yu Hou",
      "Jeremy Yeung",
      "Rui Zhang"
    ],
    "summary": "The advancement of Large Language Models (LLMs) has significantly impacted\nbiomedical Natural Language Processing (NLP), enhancing tasks such as named\nentity recognition, relation extraction, event extraction, and text\nclassification. In this context, the DeepSeek series of models have shown\npromising potential in general NLP tasks, yet their capabilities in the\nbiomedical domain remain underexplored. This study evaluates multiple DeepSeek\nmodels (Distilled-DeepSeek-R1 series and Deepseek-LLMs) across four key\nbiomedical NLP tasks using 12 datasets, benchmarking them against\nstate-of-the-art alternatives (Llama3-8B, Qwen2.5-7B, Mistral-7B, Phi-4-14B,\nGemma-2-9B). Our results reveal that while DeepSeek models perform\ncompetitively in named entity recognition and text classification, challenges\npersist in event and relation extraction due to precision-recall trade-offs. We\nprovide task-specific model recommendations and highlight future research\ndirections. This evaluation underscores the strengths and limitations of\nDeepSeek models in biomedical NLP, guiding their future deployment and\noptimization.",
    "pdf_url": "http://arxiv.org/pdf/2503.00624v1",
    "published": "2025-03-01"
  }
}